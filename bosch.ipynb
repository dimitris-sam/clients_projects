{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bcdfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.bosch-professional.com/gr/el/products/grw-18v-120-06011C2000', 'https://www.bosch-professional.com/gr/el/products/grw-18v-160-06011C3000', 'https://www.bosch-professional.com/gr/el/products/grw-12-e-06011A7000', 'https://www.bosch-professional.com/gr/el/products/grw-18-2-e-06011A8000', 'https://www.bosch-professional.com/gr/el/products/grw-140-06011C4020']\n",
      "the set detector is  Αναδευτήρας\n",
      "the set_detector_2 is  -1\n",
      "the url is  https://www.bosch-professional.com/gr/el/products/grw-18v-120-06011C2000\n",
      "Data collected for this product.\n",
      "the set detector is  Αναδευτήρας\n",
      "the set_detector_2 is  -1\n",
      "the url is  https://www.bosch-professional.com/gr/el/products/grw-18v-160-06011C3000\n",
      "Data collected for this product.\n",
      "the set detector is  Αναδευτήρας\n",
      "the set_detector_2 is  -1\n",
      "the url is  https://www.bosch-professional.com/gr/el/products/grw-12-e-06011A7000\n",
      "Data collected for this product.\n",
      "the set detector is  Αναδευτήρας\n",
      "the set_detector_2 is  -1\n",
      "the url is  https://www.bosch-professional.com/gr/el/products/grw-18-2-e-06011A8000\n",
      "Data collected for this product.\n",
      "the set detector is  Αναδευτήρας\n",
      "the set_detector_2 is  -1\n",
      "the url is  https://www.bosch-professional.com/gr/el/products/grw-140-06011C4020\n",
      "Data collected for this product.\n",
      "Data saved to bosch.xlsx\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import requests  # For sending HTTP requests to the URL\n",
    "import pandas as pd\n",
    "import re # to thelw gia ti sinartisi pou allazei selida \n",
    "\n",
    "# The URL of the website from which we want to scrape data\n",
    "url = input('vale to url')\n",
    "url = url + 'page/1/'\n",
    "# print(url)\n",
    "\n",
    "excel_file_name = input('vale to onoma tou excel arxeiou pou theleis na apothikeytoun')\n",
    "\n",
    "# Sending a GET request to the provided URL and fetching the page content\n",
    "page = requests.get(url)\n",
    "# Parsing the page content using BeautifulSoup with lxml parser\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "buttons_container = soup.find('div' , class_ = 'm-ghostblock')\n",
    "# print(buttons_container)\n",
    "pages_nr = buttons_container.find_all('a' , class_ = \"m-ghostblock__nav-item\")\n",
    "# print(len(pages_nr))\n",
    "\n",
    "# h synartisi auti allazei selida\n",
    "def increment_page_number(local_url):\n",
    "    global url\n",
    "    # Use a regular expression to find the page number pattern \"/number/\"\n",
    "    match = re.search(r'\\/(\\d+)\\/$', local_url)\n",
    "    if match:\n",
    "        # Get the current page number\n",
    "        current_page = int(match.group(1))\n",
    "        # Increment the page number\n",
    "        new_page = current_page + 1\n",
    "        # Replace the old page number with the new one in the URL\n",
    "        url = re.sub(r'\\/(\\d+)\\/$', f'/{new_page}/', local_url)\n",
    "        return url\n",
    "    else:\n",
    "        # If there's no page number, return the original URL\n",
    "        print(\"No page number found in the URL.\")\n",
    "        \n",
    "tools_url_list = []\n",
    "def collecting_urls_fun(local_url):\n",
    "    global tools_url_list\n",
    "    # Sending a GET request to the provided URL and fetching the page content\n",
    "    page = requests.get(url)\n",
    "    # Parsing the page content using BeautifulSoup with lxml parser\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    # Find the section containing the product grid (the tools)\n",
    "    tools_container = soup.find('div', class_='col-xs-12 col-lg-9')\n",
    "    # Find all individual product tiles in the grid\n",
    "    tools_list = tools_container.find_all('div', class_='category-grid-tile a-category-grid-tile')\n",
    "    # Loop through each product and extract the necessary data\n",
    "    for tool in tools_list:\n",
    "        # Extract the product URL\n",
    "        product_url = tool.find('a', class_='category-grid-tile__link-wrapper').get('href')\n",
    "        tools_url_list.append(product_url)\n",
    "\n",
    "\n",
    "tools_container = soup.find('div' , class_ = 'col-xs-12 col-lg-9')\n",
    "if tools_container == None:\n",
    "    collecting_urls_fun\n",
    "    print('there is no container')\n",
    "else:\n",
    "    tools_list = tools_container.find_all('div' , class_ = 'category-grid-tile a-category-grid-tile')\n",
    "    if len(tools_list) < 2 :\n",
    "        collecting_urls_fun(url)\n",
    "        print('there is no button')\n",
    "    else:\n",
    "        collecting_urls_fun(url)\n",
    "        # kalw loipon tin sullogi url gia prwti fora. \n",
    "        # pleon ksekinaw ti loupa. \n",
    "        for i in range(len(pages_nr)):\n",
    "            increment_page_number(url)\n",
    "            collecting_urls_fun(url)\n",
    "\n",
    "print(tools_url_list)\n",
    "    \n",
    "\n",
    "# List to hold all data\n",
    "all_data = []\n",
    "\n",
    "#  list for Σετ εργαλειων\n",
    "sets_ergaleiwn = []\n",
    "\n",
    "# Function to extract technical specifications and product code from the provided URL\n",
    "def tool_page_funct(url):\n",
    "    global all_data  # Use the global list to store data across function calls\n",
    "    global sets_ergaleiwn\n",
    "\n",
    "    # Sending a GET request to the provided URL and fetching the page content\n",
    "    page = requests.get(url)\n",
    "\n",
    "    # Parsing the page content using BeautifulSoup with lxml parser\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "    # check if the tool is a set: \n",
    "    set_detector = soup.find('p' , class_ = 'product-detail-stage__subtitle headline__redesign headline__redesign--blue')\n",
    "    set_detector = set_detector.get_text()\n",
    "    set_detector_2 = set_detector.find('Σετ')\n",
    "    print('the set detector is ' , set_detector)\n",
    "    print('the set_detector_2 is ' , set_detector_2)\n",
    "\n",
    "\n",
    "    if set_detector_2 == -1 :\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Find the section containing the product details\n",
    "            entry_specs = soup.find(\"div\", class_=\"product-detail-stage__info\")\n",
    "\n",
    "            print('the url is ' , url)\n",
    "            # Find the headline, if needed\n",
    "            praising = entry_specs.find('h3', class_='product-detail-stage__headline')\n",
    "            praising = praising.get_text() if praising else \"No headline\"\n",
    "\n",
    "            # Find the list of specifications\n",
    "            entry_specs_list = entry_specs.find('ul', class_='product-detail-stage__list a-list')\n",
    "            # Find all the <li> tags\n",
    "            li_tags = entry_specs_list.find_all('li')\n",
    "\n",
    "            # Extract the text from each <li> tag and join them with '\\n'\n",
    "            entry_specs = \"\\n\".join([li.get_text().strip() for li in li_tags])\n",
    "            entry = f\"{praising}\\n{entry_specs}\"\n",
    "            entry = [entry]  # Store the entry in a list\n",
    "\n",
    "            #  safety guard for epilogue paragraph : \n",
    "        except:\n",
    "            entry = ['None']\n",
    "\n",
    "        try: \n",
    "\n",
    "            epilogue_container = soup.find('div', class_='col-md-10')\n",
    "            # print(epilogue_container)\n",
    "            epilogue_headers = epilogue_container.find_all('h3', class_='bba--problue bba--fw-bold bba--big m-a-product_highlights__space')\n",
    "            # Extracting the text from each h3 tag\n",
    "            epilogue_headers = [feature.get_text() for feature in epilogue_headers]\n",
    "\n",
    "            epilogue_praising = epilogue_container.find_all('p')\n",
    "            epilogue_praising = [feature.get_text() for feature in epilogue_praising]\n",
    "\n",
    "            if len(epilogue_headers) != len(epilogue_praising):\n",
    "                print('There is a problem in the epilogue. The number of the headers != the number of the text (epilogue_praising)')\n",
    "            else:\n",
    "                # Now, combine headers and corresponding paragraphs into one list\n",
    "                combined_dumy_list = []\n",
    "\n",
    "                # Ensure you pair the headers with the paragraphs (assumes same number of headers and paragraphs)\n",
    "                for header, para in zip(epilogue_headers, epilogue_praising):\n",
    "                    combined_dumy_list.append(f\"{header}\\n{para}\")\n",
    "\n",
    "                # Combine all pairs into a single string, with each header-paragraph pair separated by a newline\n",
    "                epilogue_specs = \"\\n\".join(combined_dumy_list)\n",
    "\n",
    "                # Place the final output into a list as per your request\n",
    "                epilogue_specs = [epilogue_specs]\n",
    "        except:\n",
    "            print('πιθανοτατα δεν εχει καθολου παραγραφο επιλογου.')\n",
    "            epilogue_specs =['None']\n",
    "\n",
    "        # Check if variations_box exists to prevent errors\n",
    "        variations_box = soup.find('div', class_='col-sm-12 o-b-product_variations__col')\n",
    "\n",
    "        if variations_box:\n",
    "            variations = variations_box.find_all(\"div\", class_=\"col-sm-4\")\n",
    "\n",
    "            # Create lists to store part numbers, variations, items included for each variation, and technical specifications\n",
    "            part_numbers = []\n",
    "            variations_list = []\n",
    "            items_included_list = []\n",
    "\n",
    "            for container in variations:\n",
    "                # Extracting the description (variation name)\n",
    "                description = container.find(\"div\", class_=\"headline headline__redesign headline__redesign--blue headline__redesign--hl5 o-b-product_variations__productHeadline\")\n",
    "                description = description.get_text() if description else \"No description\"\n",
    "\n",
    "                # Extracting the part number (not used in the combined info column)\n",
    "                part_nr = container.find(\"p\", class_=\"o-b-product_variations__partNr\")\n",
    "                part_nr = part_nr.find('span')\n",
    "                part_nr = part_nr.get_text() if part_nr else \"No part number\"\n",
    "\n",
    "                # Extracting the items included in the package\n",
    "                including_container = container.find_all(\"ul\", class_='a-list')\n",
    "                including_list_final = []\n",
    "                for including_list in including_container:\n",
    "                    periexomena = including_list.find_all('li')\n",
    "                    item_included = [li.get_text().strip() for li in periexomena]\n",
    "                    including_list_final.append(item_included)\n",
    "\n",
    "                # Flatten the nested list\n",
    "                flattened_list = [item for sublist in including_list_final for item in sublist]\n",
    "\n",
    "                # Remove unnecessary newlines and extra spaces\n",
    "                flattened_list = [item.strip() for item in flattened_list]\n",
    "\n",
    "                # Append the description (variation), part number, and items included\n",
    "                part_numbers.append(part_nr)\n",
    "                variations_list.append(description)\n",
    "                items_included_list.append(\"\\n\".join(flattened_list))\n",
    "\n",
    "            # Extract the technical specifications only once (since it's the same for all)\n",
    "            technical_specs = soup.find('div', class_='col-lg-8 col-xs-12')\n",
    "            if technical_specs:\n",
    "                tables = technical_specs.find_all(\"table\")\n",
    "                lista = []\n",
    "                for table in tables:\n",
    "                    tbody = table.find('tbody')\n",
    "                    line = tbody.find_all(\"td\")\n",
    "                    for tr in line:\n",
    "                        text_of_line = tr.get_text().strip()\n",
    "                        lista.append(text_of_line)\n",
    "\n",
    "                # Clean the extracted data by removing newline characters\n",
    "                cleaned_data = [item.replace('\\n', '') for item in lista]\n",
    "                # Join all elements with a newline separator\n",
    "                combined_technical_specs = \"\\n\".join(cleaned_data)\n",
    "            else:\n",
    "                combined_technical_specs = \"No technical specifications available.\"\n",
    "\n",
    "            # Add the technical specifications to all rows\n",
    "            technical_specs_list = [combined_technical_specs] * len(part_numbers)\n",
    "\n",
    "            # Add the epilogue_specs to each row to match the length of part_numbers\n",
    "            epilogue_specs_list = epilogue_specs * len(part_numbers)  # Replicate for each variation\n",
    "\n",
    "            # Combine everything into a single list of dictionaries\n",
    "            for i in range(len(part_numbers)):\n",
    "                all_data.append({\n",
    "                    \"Part Number\": part_numbers[i],\n",
    "                    \"Variation\": variations_list[i],\n",
    "                    \"Items Included in the Variation\": items_included_list[i],\n",
    "                    \"Technical Specifications\": technical_specs_list[i],\n",
    "                    \"Entry Level Specs\": entry[0],  # Add entry level specs once\n",
    "                    \"Epilogue Specs\": epilogue_specs_list[i]  # Add epilogue specs for each variation\n",
    "                })\n",
    "\n",
    "            print(\"Data collected for this product.\")\n",
    "        else:\n",
    "            print(\"Could not find the variations section.\")\n",
    "\n",
    "    else:\n",
    "        print('einai set ergaleiwn pithanotata. apothikeuetai sti lista ')\n",
    "        sets_ergaleiwn.append(url)\n",
    "# # Call the function for the first time\n",
    "# tool_page_funct(url)\n",
    "\n",
    "\n",
    "for p_url in tools_url_list:\n",
    "    tool_page_funct(p_url)\n",
    "\n",
    "excel_file_name = excel_file_name + '.xlsx'\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save the data to Excel once the scraping is complete\n",
    "df.to_excel(excel_file_name , index=False)\n",
    "print(f\"Data saved to {excel_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e29dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
